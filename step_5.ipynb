{"cells":[{"cell_type":"markdown","metadata":{"id":"E2A7HpVLr5m8"},"source":["# Storage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZUNtulY0IR5"},"outputs":[],"source":["## the path to the `mldl2023` folder in your drive\n","rootMldl = '<your_drive>/mldl2023'"]},{"cell_type":"markdown","metadata":{"id":"sK-ObUBEDGAY"},"source":["# Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8pM1W1jdk05"},"outputs":[],"source":["# packages 1\n","import shutil\n","import os\n","import torch\n","\n","if not torch.cuda.is_available():\n","    raise RuntimeError('The model cannot operate without CUDA!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9eDfj3rvdnHR"},"outputs":[],"source":["# getting the notebook's name\n","from requests import get\n","from socket import gethostname, gethostbyname\n","notebookName = get(f'http://{gethostbyname(gethostname())}:9000/api/sessions').json()[0]['name']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22590,"status":"ok","timestamp":1686835380744,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"},"user_tz":-120},"id":"ykMMV6_id3Jh","outputId":"39bf91f4-4b24-4530-c386-08efb7b7771a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mounting the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bm4cdaANd-7a"},"outputs":[],"source":["# changing the root\n","os.chdir(rootMldl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47uuDoqfBx1q"},"outputs":[],"source":["# packages 2\n","import random\n","import string\n","from typing import Any\n","from typing import List\n","import numpy as np\n","from PIL import Image\n","from torch import from_numpy\n","from torchvision.datasets import VisionDataset\n","import datasets.ss_transforms as tr\n","from utils.stream_metrics import StreamSegMetrics\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","from models import deeplabv3, mobilenetv2\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torch, os, copy\n","import tqdm.notebook as tqdm\n","import gc\n","from utils.utils import HardNegativeMining, MeanReduction\n","import math\n","import json\n","import csv\n","from pprint import pprint\n","from torch import from_numpy\n","import datetime\n","import time"]},{"cell_type":"markdown","metadata":{"id":"YLz_SIPOVwfA"},"source":["# Memory Management"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5234,"status":"ok","timestamp":1686835391704,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"},"user_tz":-120},"id":"t0nrzYBUeHky","outputId":"d9234ba6-0b04-41e0-eff5-46f849f597d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gputil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: gputil\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=cf40a7455ae00be0d2cbfc9bcce3c9f8712fcc821e59d64879f40b5fbcbc8b13\n","  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.4.0\n"]}],"source":["# packages\n","!pip install gputil\n","import prettytable\n","import psutil\n","import GPUtil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkT5SH-TpS34"},"outputs":[],"source":["# garbage collector\n","def clearCache():\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CpcblaIUtxQ"},"outputs":[],"source":["# memory scanner\n","def printMemoryUsage(title='Memory status'):\n","    diskStatus = shutil.disk_usage('/content/')\n","    table = prettytable.PrettyTable(['type', 'available (MB)', 'used (MB)', 'free (MB)'])\n","    for field in table.field_names:\n","        table.align[field] = 'l'\n","    table.add_row([\n","        'disk',\n","        round((diskStatus[1]+diskStatus[2])/(1024*1024), 1),\n","        round((diskStatus[1])/(1024*1024), 1),\n","        round((diskStatus[2])/(1024*1024), 1)\n","    ])\n","    table.add_row([\n","        'ram',\n","        round((psutil.virtual_memory().available+psutil.virtual_memory().used)/(1024*1024), 1),\n","        round(psutil.virtual_memory().used/(1024*1024), 1),\n","        round(psutil.virtual_memory().available/(1024*1024), 1)\n","    ])\n","    for i, gpu in enumerate(GPUtil.getGPUs()):\n","        table.add_row([\n","            f'gpu-{i} ram',\n","            round(gpu.memoryUsed+gpu.memoryFree, 1),\n","            round(gpu.memoryUsed, 1),\n","            round(gpu.memoryFree, 1)\n","        ])\n","    print(title+':')\n","    print(table)\n","    print('')"]},{"cell_type":"markdown","metadata":{"id":"jcF7kIStWEaK"},"source":["# Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXIDEx0DaudJ"},"outputs":[],"source":["class IDDADataset(VisionDataset):\n","\n","    @staticmethod\n","    def get_mapping():\n","        classes = [255, 2, 4, 255, 11, 5, 0, 0, 1, 8, 13, 3, 7, 6, 255, 255, 15, 14, 12, 9, 10]\n","        mapping = 255*np.ones(256, dtype=np.int64)\n","        mapping[range(len(classes))] = classes\n","        return lambda x: from_numpy(mapping[x])\n","\n","    def __init__(self, root, fileNames, transform=None):\n","        super().__init__(root=root, transform=transform, target_transform=IDDADataset.get_mapping())\n","        self.fileNames = fileNames\n","\n","    def __getitem__(self, index):\n","        image = Image.open(self.root+'/images/'+self.fileNames[index]+'.jpg').convert('RGB')\n","        label = Image.open(self.root+'/labels/'+self.fileNames[index]+'.png').convert('L')\n","        if self.transform is not None:\n","            image, label = self.transform(image, label)\n","        label = self.target_transform(label)\n","        return image, label\n","\n","    def __len__(self):\n","        return len(self.fileNames)"]},{"cell_type":"markdown","metadata":{"id":"djltYWDvDl9n"},"source":["# Client class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVzH0-O7HuMu"},"outputs":[],"source":["class Client:\n","\n","    def __init__(self,\n","                 device, name,\n","                 datasetTrain,\n","                 batchSizeTrain):\n","\n","        self.device = device\n","        self.name = name\n","\n","        self.dataLoaderTrain = DataLoader(datasetTrain, batch_size=batchSizeTrain, shuffle=True, drop_last=True)\n","        self.importance = len(datasetTrain)\n","\n","    def train(self, model, num_epochs, pbar, optimizer, criterion, metric):\n","        clearCache()\n","        model.train()\n","\n","        lossCum = 0.0\n","        for epoch in range(num_epochs):\n","            lossEpoch = 0.0\n","            for batch, (images, labels) in enumerate(self.dataLoaderTrain):\n","                pbar.set_postfix({\n","                    'cluster': pbar.cluster,\n","                    'epoch': f'{epoch+1}/{num_epochs}',\n","                    'batch': f'{batch+1}/{len(self.dataLoaderTrain)}'\n","                })\n","\n","                images = images.to(self.device)\n","                labels = labels.to(self.device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(images)['out']\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                lossEpoch = (batch*lossEpoch + loss.item())/(batch + 1)\n","                _, prediction = outputs.max(dim=1)\n","                metric.update(labels.cpu().numpy(), prediction.cpu().numpy())\n","\n","                pbar.update(1)\n","\n","            lossCum = (epoch*lossCum + lossEpoch)/(epoch + 1)\n","        miouCum = metric.get_results()['Mean IoU']\n","        scoreNew = miouCum                                                      # score must be between zero and one\n","\n","        return lossCum, miouCum, scoreNew"]},{"cell_type":"markdown","metadata":{"id":"JIgFSnfk2J-3"},"source":["# Server class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZd0_ZUdB0Zd"},"outputs":[],"source":["class Server:\n","\n","    def __init__(self,\n","                 device, model, clients,\n","                 datasetTestSame, datasetTestDiff,\n","                 batchSizeTest,\n","                 metricClass, num_rounds, max_num_clients_per_round, max_num_clusters_per_round, max_num_cycles_per_cluster, max_num_epochs_per_client,\n","                 cond_allocator_neutral_score, cond_allocator_cycle_is_active, cond_allocator_epoch_is_active, cond_allocator_method,\n","                 scheduler_dict, optimizer_dict,\n","                 notebookName, pathStorage, lastRound, scores, bestRoundSame, bestMiouSame, bestRoundDiff, bestMiouDiff):\n","\n","        self.device = device\n","        self.model = model\n","        self.clients = clients\n","\n","        self.dataLoaderTestSame = DataLoader(datasetTestSame, batch_size=batchSizeTest, shuffle=True, drop_last=True)\n","        self.dataLoaderTestDiff = DataLoader(datasetTestDiff, batch_size=batchSizeTest, shuffle=True, drop_last=True)\n","\n","        self.metricClass = metricClass\n","        self.num_rounds = num_rounds\n","        self.max_num_clients_per_round = max_num_clients_per_round\n","        self.max_num_clusters_per_round = max_num_clusters_per_round\n","        self.max_num_cycles_per_cluster = max_num_cycles_per_cluster\n","        self.max_num_epochs_per_client = max_num_epochs_per_client\n","\n","        self.cond_allocator_neutral_score = cond_allocator_neutral_score\n","        self.cond_allocator_cycle_is_active = cond_allocator_cycle_is_active\n","        self.cond_allocator_epoch_is_active = cond_allocator_epoch_is_active\n","        self.cond_allocator_method = cond_allocator_method\n","\n","        # manually programmed cosine annealing scheduler\n","        self.scheduler = lambda round: scheduler_dict['lr_min'] + 0.5*(scheduler_dict['lr_initial']-scheduler_dict['lr_min'])*(1+math.cos(math.pi*round/scheduler_dict['T_max']))\n","        self.optimizer_dict = optimizer_dict\n","\n","        # STORAGE\n","        self.notebookName = notebookName                                        # STORAGE\n","        self.pathStorage = pathStorage                                          # STORAGE\n","        self.initialRound = lastRound + 1                                       # STORAGE\n","        self.scores = scores                                                    # STORAGE\n","        self.bestRoundSame = bestRoundSame                                      # STORAGE\n","        self.bestMiouSame = bestMiouSame                                        # STORAGE\n","        self.bestRoundDiff = bestRoundDiff                                      # STORAGE\n","        self.bestMiouDiff = bestMiouDiff                                        # STORAGE\n","        # STORAGE\n","\n","        self.lr = self.scheduler(self.initialRound)\n","\n","    def step(self):\n","        self.round += 1\n","        self.lr = self.scheduler(self.round)\n","\n","    def normalize_scores(self, scores):\n","        if sum([value is not None for value in scores.values()]) < 2:\n","            for key in scores:\n","                scores[key] = self.cond_allocator_neutral_score\n","            return\n","        minimum = float(np.nanmin(np.array(list(scores.values()), dtype=np.float64)))\n","        maximum = float(np.nanmax(np.array(list(scores.values()), dtype=np.float64)))\n","        if self.cond_allocator_method == 'higher-more':\n","            for key in scores:\n","                if scores[key] is None:\n","                    scores[key] = self.cond_allocator_neutral_score\n","                else:\n","                    scores[key] = (scores[key] - minimum)/(maximum - minimum)\n","        elif self.cond_allocator_method == 'lower-more':\n","            for key in scores:\n","                if scores[key] is None:\n","                    scores[key] = self.cond_allocator_neutral_score\n","                else:\n","                    scores[key] = (maximum - scores[key])/(maximum - minimum)\n","        else:\n","            raise RuntimeError(f'The normaliztion method `{self.cond_allocator_method}` does not exist!')\n","\n","    def select_clients(self):\n","        num_clients = min(self.max_num_clients_per_round, len(self.clients))\n","        return random.sample(self.clients, num_clients)\n","\n","    def assign_clients_to_clusters(self, active_clients):\n","\n","        # random clustering\n","        clustering = {}\n","        for i in range(self.max_num_clusters_per_round):\n","            cluster_name = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(5))\n","            cluster_clients =  active_clients[i::self.max_num_clusters_per_round]\n","            clustering[cluster_name] = cluster_clients\n","\n","        # normalizing the client scores\n","        scores = {}\n","        for _, cluster_clients in clustering.items():\n","            for client in cluster_clients:\n","                scores[client.name] = self.scores[client.name] if client.name in self.scores else None\n","        self.normalize_scores(scores)\n","\n","        # allocating cycles to clusters\n","        num_cycles_per_cluster = {}\n","        for cluster_name, cluster_clients in clustering.items():\n","            if self.cond_allocator_cycle_is_active:\n","                cluster_scores = []\n","                for client in cluster_clients:\n","                    cluster_scores.append(scores[client.name])\n","                num_cycles_per_cluster[cluster_name] = math.floor((self.max_num_cycles_per_cluster-1)*sum(cluster_scores)/len(cluster_scores)+1)\n","            else:\n","                num_cycles_per_cluster[cluster_name] = math.floor(self.cond_allocator_neutral_score*(self.max_num_cycles_per_cluster-1)+1)\n","\n","        # allocating epochs to eclients\n","        num_epochs_per_client = {}\n","        for _, cluster_clients in clustering.items():\n","            for client in cluster_clients:\n","                if self.cond_allocator_epoch_is_active:\n","                    num_epochs_per_client[client.name] = math.floor((self.max_num_epochs_per_client-1)*scores[client.name]+1)\n","                else:\n","                    num_epochs_per_client[client.name] = math.floor(self.cond_allocator_neutral_score*(self.max_num_epochs_per_client-1)+1)\n","\n","        # printing some info\n","        info = []\n","        for cluster_name, cluster_clients in clustering.items():\n","            cluster_info = []\n","            for client in cluster_clients:\n","                cluster_info.append(client.name)\n","            info.append(cluster_name+'->['+', '.join(cluster_info)+']')\n","        print('-- clustering: '+', '.join(info))\n","        info = []\n","        for cluster_name, num_cycles in num_cycles_per_cluster.items():\n","            info.append(f'{cluster_name}->{num_cycles}')\n","        print('-- num of cycles per cluster: '+', '.join(info))\n","        info = []\n","        for client_name, num_epochs in num_epochs_per_client.items():\n","            info.append(f'{client_name}->{num_epochs}')\n","        print('-- num of epochs per client: '+', '.join(info))\n","\n","        return clustering, num_cycles_per_cluster, num_epochs_per_client\n","\n","    def train(self):\n","        self.round = self.initialRound\n","        for round in range(self.initialRound, self.num_rounds):\n","            print(f'--------------------- round {round:03d} out of {self.num_rounds:03d} ---------------------')\n","\n","            start = time.perf_counter()\n","            lossTrain, miouTrain = self.train_round()\n","            durationTrain = int(time.perf_counter() - start)\n","            print(f'-- loss (training): {lossTrain:.5f} -- miou (training): {100*miouTrain:.3f}% -- duration (training): {durationTrain}s')\n","\n","            start = time.perf_counter()\n","            miouTestSame = self.test(self.dataLoaderTestSame)\n","            durationTestSame = int(time.perf_counter() - start)\n","            print(f'-- miou (test - same): {100*miouTestSame:.3f}% -- duration (test - same): {durationTestSame}s')\n","\n","            start = time.perf_counter()\n","            miouTestDiff = self.test(self.dataLoaderTestDiff)\n","            durationTestDiff = int(time.perf_counter() - start)\n","            print(f'-- miou (test - diff): {100*miouTestDiff:.3f}% -- duration (test - diff): {durationTestDiff}s')\n","\n","            ## STORAGE\n","            print('-- storing data')\n","            with open(self.pathStorage+'/'+'metrics.csv', 'a') as file:\n","                csv_writer = csv.writer(file)\n","                csv_writer.writerow([round, lossTrain, miouTrain, miouTestSame, miouTestDiff, durationTrain, durationTestSame, durationTestDiff])\n","                file.close()\n","\n","            with open(pathStorage+'/scores.json', 'w') as file:\n","                json.dump(self.scores, file, indent=4)\n","                file.close()\n","\n","            torch.save(self.model.state_dict(), self.pathStorage+f'/model-main.pth')\n","            models = ['main']\n","            if miouTestSame > self.bestMiouSame:\n","                self.bestRoundSame = round\n","                self.bestMiouSame = miouTestSame\n","                torch.save(self.model.state_dict(), self.pathStorage+f'/model-bestSame.pth')\n","                models.append('bestSame')\n","            if miouTestDiff > self.bestMiouDiff:\n","                self.bestRoundDiff = round\n","                self.bestMiouDiff = miouTestDiff\n","                torch.save(self.model.state_dict(), self.pathStorage+f'/model-bestDiff.pth')\n","                models.append('bestDiff')\n","\n","            with open(self.pathStorage+'/'+'log.txt', 'a') as file:\n","                file.write(f'-- models were overwritten on '+datetime.datetime.now().strftime('%Y-%m-%d at %H:%M:%S')+f' by {self.notebookName}\\n')\n","                file.write(f'   models: [{\", \".join(models)}]\\n')\n","                file.write(f'   last successful round: {round}\\n')\n","                file.write(f'   best record of same: [{self.bestRoundSame}, {self.bestMiouSame}]\\n')\n","                file.write(f'   best record of diff: [{self.bestRoundDiff}, {self.bestMiouDiff}]\\n')\n","                file.close()\n","            ## STORAGE\n","\n","    def train_round(self):\n","\n","        # clients and clusters\n","        active_clients = self.select_clients()\n","        clustering, num_cycles_per_cluster, num_epochs_per_client = self.assign_clients_to_clusters(active_clients)\n","\n","        # making a backup of the model\n","        backup = copy.deepcopy(self.model.state_dict())\n","\n","        # declaring initial values\n","        self.cluster_importances = []\n","        self.cluster_states = []\n","\n","        # round (loop over clusters)\n","        lossCum = 0.0\n","        miouCum = 0.0\n","        pbar = tqdm.tqdm(\n","            total = sum([\n","                num_cycles_per_cluster[cluster_name]*sum([\n","                    num_epochs_per_client[client.name]*len(client.dataLoaderTrain) for client in cluster_clients\n","                ]) for (cluster_name, cluster_clients) in clustering.items()\n","            ]),\n","            desc = 'training'\n","        )\n","        for cluster_index, (cluster_name, cluster_clients) in enumerate(clustering.items()):\n","\n","            # restoring the model\n","            clearCache()\n","            self.model.load_state_dict(copy.deepcopy(backup))\n","\n","            # cluster (loop over cycles)\n","            lossCluster = 0.0\n","            miouCluster = 0.0\n","            for cycle_index in range(num_cycles_per_cluster[cluster_name]):\n","\n","                # cycle (loop over clients)\n","                lossCycle = 0.0\n","                miouCycle = 0.0\n","                for client_index, client in enumerate(cluster_clients):\n","\n","                    # sending the model to the client\n","                    pbar.cluster = f'{cluster_index+1}/{len(clustering)}, cycle={cycle_index+1}/{num_cycles_per_cluster[cluster_name]}, client={client_index+1}/{len(cluster_clients)}'\n","                    loss, miou, self.scores[client.name] = client.train(\n","                        model = self.model,\n","                        num_epochs = num_epochs_per_client[client.name],\n","                        pbar = pbar,\n","                        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, **self.optimizer_dict),\n","                        criterion = nn.CrossEntropyLoss(ignore_index=255),\n","                        metric = self.metricClass(n_classes=21, name='miou')\n","                    )\n","\n","                    # cycle outputs\n","                    lossCycle = (client_index*lossCycle + loss)/(client_index + 1)\n","                    miouCycle = (client_index*miouCycle + miou)/(client_index + 1)\n","\n","                # cluster outputs\n","                lossCluster = (cycle_index*lossCluster + lossCycle)/(cycle_index + 1)\n","                miouCluster = (cycle_index*miouCluster + miouCycle)/(cycle_index + 1)\n","\n","            # storing the proposed importance of the cluster\n","            importance = 0.0\n","            for client in cluster_clients:\n","                importance += client.importance\n","            self.cluster_importances.append(importance)\n","\n","            # storing the state of the cluster\n","            state = {}\n","            for key, tensor in self.model.state_dict().items():\n","                state[key] = copy.deepcopy(tensor.to('cpu'))\n","            self.cluster_states.append(state)\n","\n","            # round outputs\n","            lossCum = (cluster_index*lossCum + lossCluster)/(cluster_index + 1)\n","            miouCum = (cluster_index*miouCum + miouCluster)/(cluster_index + 1)\n","\n","        # closing the progress bar\n","        pbar.close()\n","\n","        # stepping the scheduler\n","        self.step()\n","\n","        # calling the aggregator\n","        self.aggregate()\n","\n","        # returning the results\n","        return lossCum, miouCum\n","\n","    def aggregate(self):\n","\n","        # normalizing the cluster importances\n","        importances = torch.tensor(self.cluster_importances, dtype=torch.float, device=self.device)\n","        importances /= importances.sum()\n","\n","        # declaring an aggregate\n","        clearCache()\n","        agg = {}\n","        for key, tensor in self.model.state_dict().items():\n","            agg[key] = torch.zeros(tensor.shape, device=self.device)\n","\n","        # aggregating\n","        for i, state in enumerate(self.cluster_states):\n","            for key, tensor in state.items():\n","                agg[key] += importances[i].item()*(copy.deepcopy(tensor).to(self.device))\n","\n","        # updating the model\n","        self.model.load_state_dict(copy.deepcopy(agg))\n","\n","    def test(self, dataLoader):\n","        clearCache()\n","        metric = self.metricClass(n_classes=21, name='miou')\n","        with torch.no_grad():\n","            self.model.eval()\n","            pbar = tqdm.tqdm(total=len(dataLoader), desc='testing')\n","            for batch, (images, labels) in enumerate(dataLoader):\n","                pbar.set_postfix({\n","                    'batch': f'{batch+1}/{len(dataLoader)}'\n","                })\n","                images = images.to(self.device)\n","                labels = labels.to(self.device)\n","                outputs = self.model(images)['out']\n","                _, prediction = outputs.max(dim=1)\n","                metric.update(labels.cpu().numpy(), prediction.cpu().numpy())\n","                pbar.update(1)\n","        pbar.close()\n","        miouCum = metric.get_results()['Mean IoU']\n","        return miouCum"]},{"cell_type":"markdown","metadata":{"id":"PndQNKkiEn7G"},"source":["# Different Things"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfjsSwE2nH1_"},"outputs":[],"source":["# params class\n","class Params:\n","    def __init__(self, **args):\n","        for key, value in args.items():\n","            setattr(self, key, value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7Xaok_Jpp4a"},"outputs":[],"source":["# names reader\n","def getFileNames(root, containerName):\n","    fileNames = []\n","    with open(os.path.join(root, containerName), 'r') as file:\n","        for line in file.read().splitlines():\n","            fileNames.append(line)\n","    return fileNames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9ZfrtT9nRN-"},"outputs":[],"source":["# main function\n","def main(pathStorage, params=None, config=None):\n","    print('path: '+pathStorage)\n","\n","    # storage\n","    if os.path.exists(pathStorage):\n","        with open(pathStorage+'/log.txt') as file:\n","            lines = file.readlines()\n","            lastRoundLine = lines[-3]\n","            recordSameLine = lines[-2]\n","            recordDiffLine = lines[-1]\n","            lastRound = lastRoundLine[lastRoundLine.find(':')+2:-1]\n","            bestRoundSame = int(recordSameLine[recordSameLine.find(':')+3:recordSameLine.find(',')])\n","            bestMiouSame = float(recordSameLine[recordSameLine.find(',')+2:-2])\n","            bestRoundDiff = int(recordDiffLine[recordDiffLine.find(':')+3:recordDiffLine.find(',')])\n","            bestMiouDiff = float(recordDiffLine[recordDiffLine.find(',')+2:-2])\n","            file.close()\n","\n","        with open(pathStorage+'/params.json') as file:\n","            params = Params(**json.load(file))\n","            file.close()\n","\n","        with open(pathStorage+'/scores.json') as file:\n","            scores = json.load(file)\n","            file.close()\n","\n","        clearCache()\n","        model = deeplabv3.deeplabv3_mobilenetv2().to(params.device)\n","        model.load_state_dict(torch.load(pathStorage+'/'+'model-main.pth'))\n","\n","        with open(pathStorage+'/log.txt', 'a') as file:\n","            file.write(f'-- models were loaded on '+datetime.datetime.now().strftime('%Y-%m-%d at %H:%M:%S')+f' by {notebookName}\\n')\n","            file.write(f'   models: [main]\\n')\n","            file.write(f'   last successful round: {lastRound}\\n')\n","            file.write(recordSameLine)\n","            file.write(recordDiffLine)\n","            file.close()\n","        lastRound = -1 if lastRound == 'n/a' else int(lastRound)\n","        if (lastRound == params.num_rounds - 1):\n","            print('-- this config is finished!')\n","            return\n","\n","    else:\n","        for key, value in config.items():\n","            if hasattr(params, key):\n","                setattr(params, key, value)\n","            else:\n","                raise RuntimeError(f'Make sure each key of `config` is already an attribute of `params`! The key `{key}` does not exist!')\n","\n","        os.makedirs(pathStorage, exist_ok=True)\n","\n","        with open(pathStorage+'/params.json', 'w') as file:\n","            json.dump(params.__dict__, file, indent=4)\n","            file.close()\n","\n","        with open(pathStorage+'/config.json', 'w') as file:\n","            json.dump(dict(config), file, indent=4)\n","            file.close()\n","\n","        with open(pathStorage+'/metrics.csv', 'w', newline='') as file:\n","            csv_writer = csv.writer(file)\n","            csv_writer.writerow(['round', 'lossTrain', 'miouTrain', 'miouTestSame', 'miouTestDiff', 'durationTrain', 'durationTestSame', 'durationTestDiff'])\n","            file.close()\n","\n","        scores = {}\n","        with open(pathStorage+'/scores.json', 'w') as file:\n","            json.dump(scores, file, indent=4)\n","            file.close()\n","\n","        clearCache()\n","        model = deeplabv3.deeplabv3_mobilenetv2().to(params.device)\n","\n","        torch.save(model.state_dict(), pathStorage+'/model-main.pth')\n","        torch.save(model.state_dict(), pathStorage+'/model-bestSame.pth')\n","        torch.save(model.state_dict(), pathStorage+'/model-bestDiff.pth')\n","        with open(pathStorage+'/log.txt', 'w') as file:\n","            file.write('-- models were created on '+datetime.datetime.now().strftime('%Y-%m-%d at %H:%M:%S')+f' by {notebookName}\\n')\n","            file.write('   models: [main, bestSame, bestDiff]\\n')\n","            file.write('   last successful round: n/a\\n')\n","            file.write('   best record of same: [0, 0.0]\\n')\n","            file.write('   best record of diff: [0, 0.0]\\n')\n","            file.close()\n","        lastRound = -1\n","        bestRoundSame = 0\n","        bestMiouSame = 0.0\n","        bestRoundDiff = 0\n","        bestMiouDiff = 0.0\n","\n","    # transformers\n","    transformsTrain = tr.Compose([\n","        tr.RandomResizedCrop(size=tuple(params.transformer_imageSize),scale=params.transformer_scale),\n","        tr.ColorJitter(*params.transformer_jitter),\n","        tr.RandomHorizontalFlip(),\n","        tr.ToTensor(),\n","        tr.Normalize(tuple(params.transformer_means), tuple(params.transformer_stds))\n","    ])\n","    transformsTest = tr.Compose([\n","        tr.ToTensor(),\n","        tr.Normalize(tuple(params.transformer_means), tuple(params.transformer_stds))\n","    ])\n","\n","    # clients\n","    with open(params.rootIdda+'/train.json') as file:\n","        clientsInfo = json.load(file)\n","        file.close\n","\n","    clients = []\n","    for clientName, fileNames in clientsInfo.items():\n","        client = Client(\n","            device = params.device,\n","            name = clientName,\n","            datasetTrain = IDDADataset(\n","                root = params.rootIdda,\n","                fileNames = fileNames,\n","                transform = transformsTrain\n","            ),\n","            batchSizeTrain = params.batchSizeTrain\n","        )\n","        clients.append(client)\n","\n","    # server\n","    server = Server(\n","        device = params.device,\n","        model = model,\n","        clients = clients,\n","        datasetTestSame = IDDADataset(\n","            root = params.rootIdda,\n","            fileNames = getFileNames(params.rootIdda, 'test_same_dom.txt'),\n","            transform = transformsTest\n","        ),\n","        datasetTestDiff = IDDADataset(\n","            root = params.rootIdda,\n","            fileNames = getFileNames(params.rootIdda, 'test_diff_dom.txt'),\n","            transform = transformsTest\n","        ),\n","        batchSizeTest = params.batchSizeTest,\n","        metricClass = StreamSegMetrics,\n","        num_rounds = params.num_rounds,\n","        max_num_clients_per_round = params.max_num_clients_per_round,\n","        max_num_clusters_per_round = params.max_num_clusters_per_round,\n","        max_num_cycles_per_cluster = params.max_num_cycles_per_cluster,\n","        max_num_epochs_per_client = params.max_num_epochs_per_client,\n","        cond_allocator_neutral_score = params.cond_allocator_neutral_score,\n","        cond_allocator_cycle_is_active = params.cond_allocator_cycle_is_active,\n","        cond_allocator_epoch_is_active = params.cond_allocator_epoch_is_active,\n","        cond_allocator_method = params.cond_allocator_method,\n","        scheduler_dict = {\n","            'lr_initial': params.scheduler_lr_initial,\n","            'lr_min':  params.scheduler_lr_min,\n","            'T_max': params.scheduler_T_max\n","        },\n","        optimizer_dict = {\n","            'momentum': params.optimizer_momentum,\n","            'weight_decay': params.optimizer_weight_decay\n","        },\n","        notebookName = notebookName,                                            # STORAGE\n","        pathStorage = pathStorage,                                              # STORAGE\n","        lastRound = lastRound,                                                  # STORAGE\n","        scores = scores,                                                        # STORAGE\n","        bestRoundSame = bestRoundSame,                                          # STORAGE\n","        bestMiouSame = bestMiouSame,                                            # STORAGE\n","        bestRoundDiff = bestRoundDiff,                                          # STORAGE\n","        bestMiouDiff = bestMiouDiff                                             # STORAGE\n","    )\n","\n","    # train federated learning\n","    server.train()"]},{"cell_type":"markdown","metadata":{"id":"-9pfYAKiHVC_"},"source":["# Server Driver"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nXCNfH8n78G"},"outputs":[],"source":["# params\n","## the intitial parameters (default config)\n","## if there's a config value for a parameter, it'll be overwritten\n","params = Params(\n","    device = 'cuda:0',\n","    rootIdda = '/content/data/idda',\n","    rootStorage = rootMldl+'/storage/step5',\n","    batchSizeTrain = 3,\n","    batchSizeTest = 3,\n","    transformer_imageSize = [1080, 1920],\n","    transformer_scale = [0.25, 1],\n","    transformer_jitter = [0.4, 0.4, 0.5, 0.1],\n","    transformer_means = [0.320888, 0.292300, 0.288562],\n","    transformer_stds  = [0.250606, 0.248234, 0.253670],\n","    num_rounds = 200,                                                           # server\n","    max_num_clients_per_round = 5,                                              # server\n","    max_num_clusters_per_round = 3,                                             # server\n","    max_num_cycles_per_cluster = 1,                                             # cluster\n","    max_num_epochs_per_client = 1,                                              # client\n","    cond_allocator_neutral_score = 0.5,                                         # server (conditional allocator parameter)\n","    cond_allocator_cycle_is_active = True,                                      # server (conditional allocator parameter)\n","    cond_allocator_epoch_is_active = True,                                      # server (conditional allocator parameter)\n","    cond_allocator_method = 'higher-more',                                      # server (conditional allocator parameter)\n","    scheduler_lr_initial = 0.1,                                                 # server (scheduler parameter)\n","    scheduler_lr_min = 0,                                                       # server (scheduler parameter)\n","    scheduler_T_max = 300,                                                      # server (scheduler parameter)\n","    optimizer_momentum = 0.65,                                                  # client (optimzer parameter)\n","    optimizer_weight_decay = 0.0005                                             # client (optimzer parameter)\n",")"]},{"cell_type":"code","source":["# copies\n","print('Copying IDDA data ...')\n","shutil.copytree(rootMldl+'/data/idda', params.rootIdda, dirs_exist_ok=True)"],"metadata":{"id":"Y_8zqV_cqGpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOmFXj0kpD-6"},"outputs":[],"source":["# configs\n","## make sure each key of `config` is already an attribute of `params`\n","config = {\n","    'max_num_clients_per_round': 5,\n","    'max_num_clusters_per_round': 5,\n","    'max_num_cycles_per_cluster': 1,\n","    'max_num_epochs_per_client': 5,\n","    'cond_allocator_cycle_is_active': False,\n","    'cond_allocator_epoch_is_active': False,\n","    'cond_allocator_method': 'higher-more'\n","}\n","\n","# storage\n","folderName = []\n","for key, value in config.items():\n","    args = key.split('_')\n","    folderName.append((''.join([arg[:2].capitalize() for arg in args]))+'='+str(value))\n","folderName = ','.join(folderName)\n","\n","# driver\n","pathStorage = params.rootStorage+'/'+folderName\n","if os.path.exists(params.rootStorage) and (folderName in os.listdir(params.rootStorage)):\n","    main(pathStorage)\n","else:\n","    main(\n","        params = params,\n","        config = config,\n","        pathStorage = params.rootStorage+'/'+folderName\n","    )"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["sK-ObUBEDGAY","YLz_SIPOVwfA","jcF7kIStWEaK","djltYWDvDl9n","JIgFSnfk2J-3","PndQNKkiEn7G"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}