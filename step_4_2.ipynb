{"cells":[{"cell_type":"markdown","metadata":{"id":"E2A7HpVLr5m8"},"source":["# Storage"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WZUNtulY0IR5","executionInfo":{"status":"ok","timestamp":1686923374492,"user_tz":-120,"elapsed":14,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["## the path to the `mldl2023` folder in your drive\n","rootMldl = '<your_drive>/mldl2023'"]},{"cell_type":"markdown","metadata":{"id":"sK-ObUBEDGAY"},"source":["# Initialization"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"d8pM1W1jdk05","executionInfo":{"status":"ok","timestamp":1686923380493,"user_tz":-120,"elapsed":6012,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# packages 1\n","import shutil\n","import os\n","import torch\n","\n","if not torch.cuda.is_available():\n","    raise RuntimeError('The model cannot operate without CUDA!')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9eDfj3rvdnHR","executionInfo":{"status":"ok","timestamp":1686923380496,"user_tz":-120,"elapsed":27,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# getting the notebook's name\n","from requests import get\n","from socket import gethostname, gethostbyname\n","notebookName = get(f'http://{gethostbyname(gethostname())}:9000/api/sessions').json()[0]['name']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykMMV6_id3Jh"},"outputs":[],"source":["# mounting the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bm4cdaANd-7a","executionInfo":{"status":"ok","timestamp":1686923425484,"user_tz":-120,"elapsed":763,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# changing the root\n","os.chdir(rootMldl)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"47uuDoqfBx1q","executionInfo":{"status":"ok","timestamp":1686923427960,"user_tz":-120,"elapsed":2485,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# packages 2\n","import random\n","import string\n","from typing import Any\n","from typing import List\n","import numpy as np\n","from PIL import Image\n","from torch import from_numpy\n","from torchvision.datasets import VisionDataset\n","import datasets.ss_transforms as tr\n","from utils.stream_metrics import StreamSegMetrics\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","from models import deeplabv3, mobilenetv2\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torch, os, copy\n","import tqdm.notebook as tqdm\n","import gc\n","from utils.utils import HardNegativeMining, MeanReduction\n","import math\n","import json\n","import csv\n","from pprint import pprint\n","from torch import from_numpy\n","import datetime\n","import time"]},{"cell_type":"markdown","metadata":{"id":"YLz_SIPOVwfA"},"source":["# Memory Management"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0nrzYBUeHky"},"outputs":[],"source":["# packages\n","!pip install gputil\n","import prettytable\n","import psutil\n","import GPUtil"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"hkT5SH-TpS34","executionInfo":{"status":"ok","timestamp":1686923434258,"user_tz":-120,"elapsed":31,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# garbage collector\n","def clearCache():\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"7CpcblaIUtxQ","executionInfo":{"status":"ok","timestamp":1686923434260,"user_tz":-120,"elapsed":28,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# memory scanner\n","def printMemoryUsage(title='Memory status'):\n","    diskStatus = shutil.disk_usage('/content/')\n","    table = prettytable.PrettyTable(['type', 'available (MB)', 'used (MB)', 'free (MB)'])\n","    for field in table.field_names:\n","        table.align[field] = 'l'\n","    table.add_row([\n","        'disk',\n","        round((diskStatus[1]+diskStatus[2])/(1024*1024), 1),\n","        round((diskStatus[1])/(1024*1024), 1),\n","        round((diskStatus[2])/(1024*1024), 1)\n","    ])\n","    table.add_row([\n","        'ram',\n","        round((psutil.virtual_memory().available+psutil.virtual_memory().used)/(1024*1024), 1),\n","        round(psutil.virtual_memory().used/(1024*1024), 1),\n","        round(psutil.virtual_memory().available/(1024*1024), 1)\n","    ])\n","    for i, gpu in enumerate(GPUtil.getGPUs()):\n","        table.add_row([\n","            f'gpu-{i} ram',\n","            round(gpu.memoryUsed+gpu.memoryFree, 1),\n","            round(gpu.memoryUsed, 1),\n","            round(gpu.memoryFree, 1)\n","        ])\n","    print(title+':')\n","    print(table)\n","    print('')"]},{"cell_type":"markdown","metadata":{"id":"jcF7kIStWEaK"},"source":["# Dataset Class"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hXIDEx0DaudJ","executionInfo":{"status":"ok","timestamp":1686923434261,"user_tz":-120,"elapsed":26,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["class IDDADataset(VisionDataset):\n","\n","    @staticmethod\n","    def get_mapping():\n","        classes = [255, 2, 4, 255, 11, 5, 0, 0, 1, 8, 13, 3, 7, 6, 255, 255, 15, 14, 12, 9, 10]\n","        mapping = 255*np.ones(256, dtype=np.int64)\n","        mapping[range(len(classes))] = classes\n","        return lambda x: from_numpy(mapping[x])\n","\n","    def __init__(self, root, fileNames, transform=None):\n","        super().__init__(root=root, transform=transform, target_transform=IDDADataset.get_mapping())\n","        self.fileNames = fileNames\n","\n","    def __getitem__(self, index):\n","        image = Image.open(self.root+'/images/'+self.fileNames[index]+'.jpg').convert('RGB')\n","        label = Image.open(self.root+'/labels/'+self.fileNames[index]+'.png').convert('L')\n","        if self.transform is not None:\n","            image, label = self.transform(image, label)\n","        label = self.target_transform(label)\n","        return image, label\n","\n","    def __len__(self):\n","        return len(self.fileNames)"]},{"cell_type":"markdown","source":["# Pseudo-Label Functions"],"metadata":{"id":"lWoG95IrMu86"}},{"cell_type":"code","source":["def get_image_mask(prob, pseudo_lab, th):\n","    #.clone to copy\n","    # .detach to take out the computations behind the tensor, like gradient track\n","    # .max(0) to get max on dim 0, and [0] to get the probability\n","    #  max(0) is a tuple, first item is the highest probability, the second is the\n","    # index of the highest probability (on dim=0)\n","    max_prob = prob.detach().clone().max(0)[0]\n","\n","    # put a mask where the highest probability is higher than th\n","    mask_prob = max_prob > th\n","    return mask_prob"],"metadata":{"id":"1o5d6FFEM1ZK","executionInfo":{"status":"ok","timestamp":1686923434263,"user_tz":-120,"elapsed":25,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_batch_mask(pred, pseudo_lab, th):\n","    import torch.nn.functional as F\n","    # normalize (using softmax) the probabilities for each class for each pixel\n","    # for each image in batch\n","    # and give the masks\n","    # (got this line from TA code)\n","    mask = torch.stack([get_image_mask(pb, pl, th) for pb, pl in zip(F.softmax(pred, dim=1), pseudo_lab)], dim=0)\n","    return mask"],"metadata":{"id":"Up8GsAH5M4xq","executionInfo":{"status":"ok","timestamp":1686923434264,"user_tz":-120,"elapsed":24,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def get_pseudo_lab(imgs, model, th):\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(imgs)\n","    # output[\"out\"] returns the probabilities of each class\n","    pred = output[\"out\"]\n","\n","    # max(1) is a tuple, first item is the highest probability, the second is the\n","    # index of the highest probability\n","    # (on dim=1)\n","    # .detach() takes out the gradient computations and etc\n","    pseudo_lab = pred.detach().max(1)[1]\n","\n","    mask = get_batch_mask(pred, pseudo_lab, th)\n","\n","    # change the pixels where the max probability is under\n","    # the threshold to 255, that is ignored in the loss computation\n","    pseudo_lab[~mask] = 255\n","    return pseudo_lab"],"metadata":{"id":"f152quMcM6Gp","executionInfo":{"status":"ok","timestamp":1686923434792,"user_tz":-120,"elapsed":551,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djltYWDvDl9n"},"source":["# Client class"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"PVzH0-O7HuMu","executionInfo":{"status":"ok","timestamp":1686923434794,"user_tz":-120,"elapsed":36,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["class Client:\n","\n","    def __init__(self,\n","                 device, name,\n","                 datasetTrain,\n","                 batchSizeTrain):\n","\n","        self.device = device\n","        self.name = name\n","\n","        self.dataLoaderTrain = DataLoader(datasetTrain, batch_size=batchSizeTrain, shuffle=True, drop_last=True)\n","        self.importance = len(datasetTrain)\n","\n","    def train(self, modelTeacher, modelStudent, num_epochs, pbar, threshold, optimizer, criterion, metric):\n","        clearCache()\n","        modelStudent.train()\n","\n","        lossCum = 0.0\n","        for epoch in range(num_epochs):\n","            lossEpoch = 0.0\n","            for batch, (images, _) in enumerate(self.dataLoaderTrain):\n","                pbar.set_postfix({\n","                    'client': pbar.client,\n","                    'epoch': f'{epoch+1}/{num_epochs}',\n","                    'batch': f'{batch+1}/{len(self.dataLoaderTrain)}'\n","                })\n","\n","                images = images.to(self.device)\n","                pseudoLabels = get_pseudo_lab(images, modelTeacher, threshold).to(self.device)\n","\n","                optimizer.zero_grad()\n","                outputs = modelStudent(images)['out']\n","                loss = criterion(outputs, pseudoLabels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                lossEpoch = (batch*lossEpoch + loss.item())/(batch + 1)\n","                _, prediction = outputs.max(dim=1)\n","                metric.update(pseudoLabels.cpu().numpy(), prediction.cpu().numpy())\n","\n","                pbar.update(1)\n","\n","            lossCum = (epoch*lossCum + lossEpoch)/(epoch + 1)\n","        miouCum = metric.get_results()['Mean IoU']\n","\n","        return lossCum, miouCum"]},{"cell_type":"markdown","metadata":{"id":"JIgFSnfk2J-3"},"source":["# Server class"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"JZd0_ZUdB0Zd","executionInfo":{"status":"ok","timestamp":1686923434795,"user_tz":-120,"elapsed":35,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["class Server:\n","\n","    def __init__(self,\n","                 device, modelTeacher, modelStudent, clients,\n","                 datasetTestIdda, datasetTestSame, datasetTestDiff,\n","                 batchSizeTest,\n","                 metricClass, num_rounds, max_num_clients_per_round, max_num_epochs_per_client, threshold, updatePeriod,\n","                 scheduler_dict, optimizer_dict,\n","                 notebookName, pathStorage, lastRound):\n","\n","        self.device = device\n","        self.modelTeacher = modelTeacher\n","        self.modelStudent = modelStudent\n","        self.clients = clients\n","\n","        self.dataLoaderTestIdda = DataLoader(datasetTestIdda, batch_size=batchSizeTest, shuffle=False, drop_last=False)\n","        self.dataLoaderTestSame = DataLoader(datasetTestSame, batch_size=batchSizeTest, shuffle=False, drop_last=False)\n","        self.dataLoaderTestDiff = DataLoader(datasetTestDiff, batch_size=batchSizeTest, shuffle=False, drop_last=False)\n","\n","        self.metricClass = metricClass\n","        self.num_rounds = num_rounds\n","        self.max_num_clients_per_round = max_num_clients_per_round\n","        self.max_num_epochs_per_client = max_num_epochs_per_client\n","        self.threshold = threshold\n","        self.updatePeriod = updatePeriod\n","\n","\n","        # manually programmed cosine annealing scheduler\n","        self.scheduler = lambda round: scheduler_dict['lr_min'] + 0.5*(scheduler_dict['lr_initial']-scheduler_dict['lr_min'])*(1+math.cos(math.pi*round/scheduler_dict['T_max']))\n","        self.optimizer_dict = optimizer_dict\n","\n","        # STORAGE\n","        self.notebookName = notebookName                                        # STORAGE\n","        self.pathStorage = pathStorage                                          # STORAGE\n","        self.initialRound = lastRound + 1                                       # STORAGE\n","        # STORAGE\n","\n","        self.lr = self.scheduler(self.initialRound)\n","\n","    def step(self):\n","        self.round += 1\n","        self.lr = self.scheduler(self.round)\n","\n","    def select_clients(self):\n","        num_clients = min(self.max_num_clients_per_round, len(self.clients))\n","        return random.sample(self.clients, num_clients)\n","\n","    def train(self):\n","        self.round = self.initialRound\n","        for round in range(self.initialRound, self.num_rounds):\n","            print(f'--------------------- round {round:03d} out of {self.num_rounds:03d} ---------------------')\n","\n","            start = time.perf_counter()\n","            lossTrain, miouTrain = self.train_round()\n","            durationTrain = int(time.perf_counter() - start)\n","            print(f'-- loss (training): {lossTrain:.5f} -- miou (training): {100*miouTrain:.3f}% -- duration (training): {durationTrain}s')\n","\n","            start = time.perf_counter()\n","            miouTestIdda = self.test(self.dataLoaderTestIdda)\n","            durationTestIdda = int(time.perf_counter() - start)\n","            print(f'-- miou (test - idda): {100*miouTestIdda:.3f}% -- duration (test - idda): {durationTestIdda}s')\n","\n","            start = time.perf_counter()\n","            miouTestSame = self.test(self.dataLoaderTestSame)\n","            durationTestSame = int(time.perf_counter() - start)\n","            print(f'-- miou (test - same): {100*miouTestSame:.3f}% -- duration (test - same): {durationTestSame}s')\n","\n","            start = time.perf_counter()\n","            miouTestDiff = self.test(self.dataLoaderTestDiff)\n","            durationTestDiff = int(time.perf_counter() - start)\n","            print(f'-- miou (test - diff): {100*miouTestDiff:.3f}% -- duration (test - diff): {durationTestDiff}s')\n","\n","            ## STORAGE\n","            print('-- storing data')\n","            with open(self.pathStorage+'/'+'metrics.csv', 'a') as file:\n","                csv_writer = csv.writer(file)\n","                csv_writer.writerow([round, lossTrain, miouTrain, miouTestSame, miouTestIdda, miouTestDiff, durationTrain, durationTestIdda, durationTestSame, durationTestDiff])\n","                file.close()\n","\n","            torch.save(self.modelTeacher.state_dict(), self.pathStorage+f'/model-teacher.pth')\n","            torch.save(self.modelStudent.state_dict(), self.pathStorage+f'/model-student.pth')\n","\n","            with open(self.pathStorage+'/'+'log.txt', 'a') as file:\n","                file.write(f'-- models were overwritten on '+datetime.datetime.now().strftime('%Y-%m-%d at %H:%M:%S')+f' by {self.notebookName}\\n')\n","                file.write(f'   models: [teacher, student]\\n')\n","                file.write(f'   last successful round: {round}\\n')\n","                file.close()\n","            ## STORAGE\n","\n","    def train_round(self):\n","\n","        # clients\n","        active_clients = self.select_clients()\n","        num_epochs_per_client = {client.name:self.max_num_epochs_per_client for client in active_clients}\n","\n","        # updating the teacher model\n","        if self.updatePeriod != 0:\n","            if (self.round % self.updatePeriod) == 0:\n","                self.modelTeacher.load_state_dict(copy.deepcopy(self.modelStudent.state_dict()))\n","\n","        # making a backup of the student model\n","        backup = copy.deepcopy(self.modelStudent.state_dict())\n","\n","        # declaring initial values\n","        self.client_importances = []\n","        self.client_states = []\n","\n","        # round (loop over clients)\n","        lossCum = 0.0\n","        miouCum = 0.0\n","        pbar = tqdm.tqdm(\n","            total = sum([num_epochs_per_client[client.name]*len(client.dataLoaderTrain) for client in active_clients]),\n","            desc = 'training'\n","        )\n","        for client_index, client in enumerate(active_clients):\n","\n","            # restoring the student model\n","            clearCache()\n","            self.modelStudent.load_state_dict(copy.deepcopy(backup))\n","\n","            # sending the model to the client\n","            pbar.client = f'{client_index+1}/{len(active_clients)}'\n","            loss, miou = client.train(\n","                modelTeacher = self.modelTeacher,\n","                modelStudent = self.modelStudent,\n","                num_epochs = num_epochs_per_client[client.name],\n","                pbar = pbar,\n","                threshold = self.threshold,\n","                optimizer = torch.optim.SGD(self.modelStudent.parameters(), lr=self.lr, **self.optimizer_dict),\n","                criterion = nn.CrossEntropyLoss(ignore_index=255),\n","                metric = self.metricClass(n_classes=21, name='miou')\n","            )\n","\n","            # round outputs\n","            lossCum = (client_index*lossCum + loss)/(client_index + 1)\n","            miouCum = (client_index*miouCum + miou)/(client_index + 1)\n","\n","            # storing the proposed importance of the client\n","            self.client_importances.append(float(client.importance))\n","\n","            # storing the state of the client\n","            state = {}\n","            for key, tensor in self.modelStudent.state_dict().items():\n","                state[key] = copy.deepcopy(tensor.to('cpu'))\n","            self.client_states.append(state)\n","\n","        # closing the progress bar\n","        pbar.close()\n","\n","        # stepping the scheduler\n","        self.step()\n","\n","        # calling the aggregator\n","        self.aggregate()\n","\n","        # returning the results\n","        return lossCum, miouCum\n","\n","    def aggregate(self):\n","\n","        # normalizing the client importances\n","        importances = torch.tensor(self.client_importances, dtype=torch.float, device=self.device)\n","        importances /= importances.sum()\n","\n","        # declaring an aggregate\n","        clearCache()\n","        agg = {}\n","        for key, tensor in self.modelStudent.state_dict().items():\n","            agg[key] = torch.zeros(tensor.shape, device=self.device)\n","\n","        # aggregating\n","        for i, state in enumerate(self.client_states):\n","            for key, tensor in state.items():\n","                agg[key] += importances[i].item()*(copy.deepcopy(tensor).to(self.device))\n","\n","        # updating the model\n","        self.modelStudent.load_state_dict(copy.deepcopy(agg))\n","\n","    def test(self, dataLoader):\n","        clearCache()\n","        metric = self.metricClass(n_classes=21, name='miou')\n","        with torch.no_grad():\n","            self.modelStudent.eval()\n","            pbar = tqdm.tqdm(total=len(dataLoader), desc='testing')\n","            for batch, (images, labels) in enumerate(dataLoader):\n","                pbar.set_postfix({\n","                    'batch': f'{batch+1}/{len(dataLoader)}'\n","                })\n","                images = images.to(self.device)\n","                labels = labels.to(self.device)\n","                outputs = self.modelStudent(images)['out']\n","                _, prediction = outputs.max(dim=1)\n","                metric.update(labels.cpu().numpy(), prediction.cpu().numpy())\n","                pbar.update(1)\n","        pbar.close()\n","        miouCum = metric.get_results()['Mean IoU']\n","        return miouCum"]},{"cell_type":"markdown","metadata":{"id":"PndQNKkiEn7G"},"source":["# Different Things"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"SfjsSwE2nH1_","executionInfo":{"status":"ok","timestamp":1686923434796,"user_tz":-120,"elapsed":35,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# params class\n","class Params:\n","    def __init__(self, **args):\n","        for key, value in args.items():\n","            setattr(self, key, value)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"d7Xaok_Jpp4a","executionInfo":{"status":"ok","timestamp":1686923434798,"user_tz":-120,"elapsed":35,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# names reader\n","def getFileNames(root, containerName):\n","    fileNames = []\n","    with open(os.path.join(root, containerName), 'r') as file:\n","        for line in file.read().splitlines():\n","            fileNames.append(line)\n","    return fileNames"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"t9ZfrtT9nRN-","executionInfo":{"status":"ok","timestamp":1686923434799,"user_tz":-120,"elapsed":35,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# main function\n","def main(pathStorage, params=None, config=None):\n","    print('path: '+pathStorage)\n","\n","    # storage\n","    if os.path.exists(pathStorage):\n","        with open(pathStorage+'/log.txt') as file:\n","            lines = file.readlines()\n","            lastRoundLine = lines[-1]\n","            lastRound = lastRoundLine[lastRoundLine.find(':')+2:-1]\n","            file.close()\n","\n","        with open(pathStorage+'/params.json') as file:\n","            params = Params(**json.load(file))\n","            file.close()\n","\n","        clearCache()\n","        modelTeacher = deeplabv3.deeplabv3_mobilenetv2().to(params.device)\n","        modelStudent = deeplabv3.deeplabv3_mobilenetv2().to(params.device)\n","        modelTeacher.load_state_dict(torch.load(pathStorage+'/'+'model-teacher.pth'))\n","        modelStudent.load_state_dict(torch.load(pathStorage+'/'+'model-student.pth'))\n","\n","        with open(pathStorage+'/log.txt', 'a') as file:\n","            file.write(f'-- models were loaded on '+datetime.datetime.now().strftime('%Y-%m-%d at %H:%M:%S')+f' by {notebookName}\\n')\n","            file.write(f'   models: [teacher, student]\\n')\n","            file.write(f'   last successful round: {lastRound}\\n')\n","            file.close()\n","        lastRound = -1 if lastRound == 'n/a' else int(lastRound)\n","        if (lastRound == params.num_rounds - 1):\n","            print('-- this config is finished!')\n","            return\n","\n","    else:\n","        for key, value in config.items():\n","            if hasattr(params, key):\n","                setattr(params, key, value)\n","            else:\n","                raise RuntimeError(f'Make sure each key of `config` is already an attribute of `params`! The key `{key}` does not exist!')\n","\n","        os.makedirs(pathStorage, exist_ok=True)\n","\n","        with open(pathStorage+'/params.json', 'w') as file:\n","            json.dump(params.__dict__, file, indent=4)\n","            file.close()\n","\n","        with open(pathStorage+'/config.json', 'w') as file:\n","            json.dump(dict(config), file, indent=4)\n","            file.close()\n","\n","        with open(pathStorage+'/metrics.csv', 'w', newline='') as file:\n","            csv_writer = csv.writer(file)\n","            csv_writer.writerow(['round', 'lossTrain', 'miouTrain', 'miouIdda', 'miouTestSame', 'miouTestDiff', 'durationTrain', 'durationIdda', 'durationTestSame', 'durationTestDiff'])\n","            file.close()\n","\n","        clearCache()\n","        modelTeacher = deeplabv3.deeplabv3_mobilenetv2().to(params.device)\n","        modelStudent = deeplabv3.deeplabv3_mobilenetv2().to(params.device)\n","        modelTeacher.load_state_dict(torch.load(params.rootPretrained+'/model_pretrained_step_3_2.pth'))\n","        modelStudent.load_state_dict(torch.load(params.rootPretrained+'/model_pretrained_step_3_2.pth'))\n","\n","        torch.save(modelTeacher.state_dict(), pathStorage+'/model-teacher.pth')\n","        torch.save(modelStudent.state_dict(), pathStorage+'/model-student.pth')\n","        with open(pathStorage+'/log.txt', 'w') as file:\n","            file.write('-- models were created on '+datetime.datetime.now().strftime('%Y-%m-%d at %H:%M:%S')+f' by {notebookName}\\n')\n","            file.write('   models: [teacher, student]\\n')\n","            file.write('   last successful round: n/a\\n')\n","            file.close()\n","        lastRound = -1\n","\n","    # transformers\n","    transformsTrain = tr.Compose([\n","        tr.RandomResizedCrop(size=tuple(params.transformer_imageSize),scale=params.transformer_scale),\n","        tr.ColorJitter(*params.transformer_jitter),\n","        tr.RandomHorizontalFlip(),\n","        tr.ToTensor(),\n","        tr.Normalize(tuple(params.transformer_means), tuple(params.transformer_stds))\n","    ])\n","    transformsTest = tr.Compose([\n","        tr.ToTensor(),\n","        tr.Normalize(tuple(params.transformer_means), tuple(params.transformer_stds))\n","    ])\n","\n","    # clients\n","    with open(params.rootIdda+'/train.json') as file:\n","        clientsInfo = json.load(file)\n","        file.close\n","\n","    clients = []\n","    for clientName, fileNames in clientsInfo.items():\n","        client = Client(\n","            device = params.device,\n","            name = clientName,\n","            datasetTrain = IDDADataset(\n","                root = params.rootIdda,\n","                fileNames = fileNames,\n","                transform = transformsTrain\n","            ),\n","            batchSizeTrain = params.batchSizeTrain\n","        )\n","        clients.append(client)\n","\n","    # server\n","    server = Server(\n","        device = params.device,\n","        modelTeacher = modelTeacher,\n","        modelStudent = modelStudent,\n","        clients = clients,\n","        datasetTestIdda = IDDADataset(\n","            root = params.rootIdda,\n","            fileNames = getFileNames(params.rootIdda, 'train.txt'),\n","            transform = transformsTest\n","        ),\n","        datasetTestSame = IDDADataset(\n","            root = params.rootIdda,\n","            fileNames = getFileNames(params.rootIdda, 'test_same_dom.txt'),\n","            transform = transformsTest\n","        ),\n","        datasetTestDiff = IDDADataset(\n","            root = params.rootIdda,\n","            fileNames = getFileNames(params.rootIdda, 'test_diff_dom.txt'),\n","            transform = transformsTest\n","        ),\n","        batchSizeTest = params.batchSizeTest,\n","        metricClass = StreamSegMetrics,\n","        num_rounds = params.num_rounds,\n","        max_num_clients_per_round = params.max_num_clients_per_round,\n","        max_num_epochs_per_client = params.max_num_epochs_per_client,\n","        threshold = params.threshold,\n","        updatePeriod = params.updatePeriod,\n","        scheduler_dict = {\n","            'lr_initial': params.scheduler_lr_initial,\n","            'lr_min':  params.scheduler_lr_min,\n","            'T_max': params.scheduler_T_max\n","        },\n","        optimizer_dict = {\n","            'momentum': params.optimizer_momentum,\n","            'weight_decay': params.optimizer_weight_decay\n","        },\n","        notebookName = notebookName,                                            # STORAGE\n","        pathStorage = pathStorage,                                              # STORAGE\n","        lastRound = lastRound                                                   # STORAGE\n","    )\n","\n","    # train federated learning\n","    server.train()"]},{"cell_type":"markdown","metadata":{"id":"-9pfYAKiHVC_"},"source":["# Server Run"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"7nXCNfH8n78G","executionInfo":{"status":"ok","timestamp":1686923434800,"user_tz":-120,"elapsed":34,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}}},"outputs":[],"source":["# params\n","## the intitial parameters (default config)\n","## if there's a config value for a parameter, it'll be overwritten\n","params = Params(\n","    device = 'cuda:0',\n","    rootIdda = '/content/data/idda',\n","    rootPretrained = rootMldl+'/pretrained',\n","    rootStorage = rootMldl+'/storage/step4/part2',\n","    batchSizeTrain = 3,\n","    batchSizeTest = 3,\n","    transformer_imageSize = [1080, 1920],\n","    transformer_scale = [0.25, 1],\n","    transformer_jitter = [0.4, 0.4, 0.5, 0.1],\n","    transformer_means = [0.320888, 0.292300, 0.288562],\n","    transformer_stds  = [0.250606, 0.248234, 0.253670],\n","    num_rounds = 20,                                                            # server\n","    max_num_clients_per_round = 7,                                              # server\n","    max_num_epochs_per_client = 5,                                              # client\n","    threshold = 0.9,                                                            # server (teacher model threshold)\n","    updatePeriod = 10,                                                          # server (teacher model update period; 0 means no update)\n","    scheduler_lr_initial = 0.01,                                                # server (scheduler parameter)\n","    scheduler_lr_min = 0,                                                       # server (scheduler parameter)\n","    scheduler_T_max = 30,                                                       # server (scheduler parameter)\n","    optimizer_momentum = 0.65,                                                  # client (optimzer parameter)\n","    optimizer_weight_decay = 0.0005                                             # client (optimzer parameter)\n",")"]},{"cell_type":"code","source":["# copies\n","print('Copying IDDA data ...')\n","shutil.copytree(rootMldl+'/data/idda', params.rootIdda, dirs_exist_ok=True)"],"metadata":{"id":"cCyeE-Ijrbza"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOmFXj0kpD-6"},"outputs":[],"source":["# configs\n","## make sure each key of `config` is already an attribute of `params`\n","config = {\n","    'updatePeriod': 10,\n","    'max_num_clients_per_round': 8,\n","    'max_num_epochs_per_client': 1,\n","}\n","\n","# storage\n","folderName = []\n","for key, value in config.items():\n","    args = key.split('_')\n","    folderName.append((''.join([arg[:2].capitalize() for arg in args]))+'='+str(value))\n","folderName = ','.join(folderName)\n","\n","# driver\n","pathStorage = params.rootStorage+'/'+folderName\n","if os.path.exists(params.rootStorage) and (folderName in os.listdir(params.rootStorage)):\n","    main(pathStorage)\n","else:\n","    main(\n","        params = params,\n","        config = config,\n","        pathStorage = params.rootStorage+'/'+folderName\n","    )"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["sK-ObUBEDGAY","YLz_SIPOVwfA","jcF7kIStWEaK","lWoG95IrMu86","djltYWDvDl9n","JIgFSnfk2J-3","PndQNKkiEn7G"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}